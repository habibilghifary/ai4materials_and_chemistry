{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b0fc32",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression (GPR) with UQ â€” Tuned (skopt)\n",
    "\n",
    "This notebook performs Gaussian Process Regression on the uploaded\n",
    "`concrete_data.csv` dataset (subsampled to 400 points for speed) and\n",
    "performs hyperparameter tuning using scikit-optimize (`skopt`) with\n",
    "two objectives: CV-RMSE (BayesSearchCV) and negative log marginal likelihood (gp_minimize).\n",
    "\n",
    "Run **Kernel -> Restart & Run All** in Jupyter or **Runtime -> Run all** in Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cf7f35",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Install required packages if missing:\n",
    "\n",
    "```\n",
    "pip install numpy pandas scikit-learn matplotlib seaborn scikit-optimize scipy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f125dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset (uploaded by you)\n",
    "import pandas as pd, numpy as np\n",
    "file_path = \"/mnt/data/concrete_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(\"Loaded:\", file_path, \"shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de03bc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Subsample to at most 400 samples\n",
    "max_samples = 400\n",
    "if len(df) > max_samples:\n",
    "    df = df.sample(n=max_samples, random_state=42).reset_index(drop=True)\n",
    "    print(\"Subsampled to\", len(df))\n",
    "else:\n",
    "    print(\"Using full dataset of\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c273f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess: train/test split and scaling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "print(\"Train/test shapes:\", X_train_s.shape, X_test_s.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Baseline GPR (internal LML tuning)\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, Matern, WhiteKernel\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "kernel = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=1.5) + WhiteKernel(noise_level=1.0)\n",
    "gpr_base = GaussianProcessRegressor(kernel=kernel, normalize_y=True, n_restarts_optimizer=2, random_state=0)\n",
    "print('Fitting baseline GPR...')\n",
    "gpr_base.fit(X_train_s, y_train)\n",
    "y_pred_base, y_std_base = gpr_base.predict(X_test_s, return_std=True)\n",
    "print('Baseline kernel:', gpr_base.kernel_)\n",
    "print('Baseline RMSE:', sqrt(mean_squared_error(y_test, y_pred_base)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e40f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CV-based tuning via BayesSearchCV (optimize CV RMSE)\n",
    "try:\n",
    "    from skopt import BayesSearchCV\n",
    "    from skopt.space import Real, Integer, Categorical\n",
    "    from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "    from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel\n",
    "    from math import sqrt\n",
    "    from sklearn.metrics import make_scorer, mean_squared_error\n",
    "    \n",
    "    print('Running BayesSearchCV to optimize CV RMSE...')\n",
    "    # Define a param space in terms of kernel objects for coarse search\n",
    "    def kernel_from_params(ls, sv, nl):\n",
    "        return ConstantKernel(sv) * RBF(length_scale=ls) + WhiteKernel(noise_level=nl)\n",
    "    \n",
    "    # We'll search continuous parameters by encoding them and constructing kernel in the estimator via a custom wrapper below.\n",
    "    # For simplicity here, we use BayesSearchCV over kernel objects (coarse) and also provide an example of continuous search via gp_minimize later.\n",
    "    k1 = kernel_from_params(1.0, 1.0, 1e-2)\n",
    "    k2 = kernel_from_params(0.1, 2.0, 1e-3)\n",
    "    k3 = kernel_from_params(5.0, 0.5, 1e-1)\n",
    "    gpr_for_search = GaussianProcessRegressor(normalize_y=True, n_restarts_optimizer=0)\n",
    "    search_spaces = {'kernel': Categorical([k1, k2, k3])}\n",
    "    bayes_cv = BayesSearchCV(gpr_for_search, search_spaces, scoring='neg_root_mean_squared_error', n_iter=12, cv=4, random_state=0, n_jobs=1)\n",
    "    bayes_cv.fit(X_train_s, y_train)\n",
    "    print('BayesSearchCV best score:', bayes_cv.best_score_)\n",
    "    print('Best kernel (CV-RMSE):', bayes_cv.best_estimator_.kernel_)\n",
    "    gpr_cv = bayes_cv.best_estimator_\n",
    "    y_pred_cv, y_std_cv = gpr_cv.predict(X_test_s, return_std=True)\n",
    "    print('Test RMSE (CV-optimized):', sqrt(mean_squared_error(y_test, y_pred_cv)))\n",
    "except Exception as e:\n",
    "    print('BayesSearchCV failed or skopt not installed:', e)\n",
    "    gpr_cv = gpr_base\n",
    "    y_pred_cv, y_std_cv = y_pred_base, y_std_base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e536737",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NLL tuning via gp_minimize: directly minimize negative log marginal likelihood on training data\n",
    "try:\n",
    "    from skopt import gp_minimize\n",
    "    from skopt.space import Real\n",
    "    from sklearn.gaussian_process.kernels import RBF, ConstantKernel, WhiteKernel\n",
    "    from math import sqrt\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    \n",
    "    print('Running gp_minimize for NLL optimization (this will fit many GPRs)...')\n",
    "    space = [Real(1e-2, 10.0, name='length_scale'), Real(1e-2, 10.0, name='signal_var'), Real(1e-6, 1e-1, name='noise_level')]\n",
    "    \n",
    "    def objective(params):\n",
    "        ls, sv, nl = params\n",
    "        kernel = ConstantKernel(sv) * RBF(length_scale=ls) + WhiteKernel(noise_level=nl)\n",
    "        gpr = GaussianProcessRegressor(kernel=kernel, normalize_y=True, optimizer=None)  # no internal optimizer for fairness\n",
    "        gpr.fit(X_train_s, y_train)\n",
    "        return -gpr.log_marginal_likelihood_value_\n",
    "    \n",
    "    res = gp_minimize(objective, space, n_calls=30, random_state=0, n_initial_points=8)\n",
    "    best_ls, best_sv, best_nl = res.x\n",
    "    print('Best params (NLL):', res.x, 'best NLL:', res.fun)\n",
    "    best_kernel = ConstantKernel(best_sv) * RBF(length_scale=best_ls) + WhiteKernel(noise_level=best_nl)\n",
    "    gpr_nll = GaussianProcessRegressor(kernel=best_kernel, normalize_y=True, n_restarts_optimizer=0, random_state=0)\n",
    "    gpr_nll.fit(X_train_s, y_train)\n",
    "    y_pred_nll, y_std_nll = gpr_nll.predict(X_test_s, return_std=True)\n",
    "    print('Test RMSE (NLL-optimized):', sqrt(mean_squared_error(y_test, y_pred_nll)))\n",
    "except Exception as e:\n",
    "    print('gp_minimize failed or skopt not installed:', e)\n",
    "    gpr_nll = gpr_base\n",
    "    y_pred_nll, y_std_nll = y_pred_base, y_std_base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6cfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compare baseline, CV-optimized, and NLL-optimized models\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "def mean_nll(y_true, y_pred, y_std):\n",
    "    var = y_std**2 + 1e-12\n",
    "    nll = 0.5 * ((y_true - y_pred)**2 / var + np.log(2*np.pi*var))\n",
    "    return float(np.mean(nll))\n",
    "\n",
    "def summarise(name, yp, ys):\n",
    "    rmse = sqrt(mean_squared_error(y_test, yp))\n",
    "    mae = mean_absolute_error(y_test, yp)\n",
    "    nll = mean_nll(y_test, yp, ys)\n",
    "    print(f'{name}: RMSE={rmse:.4f}, MAE={mae:.4f}, mean NLL={nll:.4f}')\n",
    "\n",
    "print('Model comparison:')\n",
    "summarise('Baseline', y_pred_base, y_std_base)\n",
    "summarise('CV-optimized', y_pred_cv, y_std_cv)\n",
    "summarise('NLL-optimized', y_pred_nll, y_std_nll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d337c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualization: parity, uncertainty vs error, std distribution (for NLL-optimized model)\n",
    "import matplotlib.pyplot as plt, seaborn as sns, numpy as np\n",
    "sns.set(style='whitegrid')\n",
    "# Parity plot\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y_test, y_pred_nll, alpha=0.6)\n",
    "m = min(min(y_test), min(y_pred_nll)); M = max(max(y_test), max(y_pred_nll))\n",
    "plt.plot([m,M],[m,M],'r--'); plt.xlabel('True'); plt.ylabel('Predicted'); plt.title('Parity: NLL-optimized')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Uncertainty vs absolute error\n",
    "err = np.abs(y_test - y_pred_nll)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(y_std_nll, err, alpha=0.7)\n",
    "plt.xlabel('Predictive std'); plt.ylabel('Absolute error'); plt.title('Uncertainty vs Absolute Error')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Std distribution\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(y_std_nll, bins=30, kde=True)\n",
    "plt.xlabel('Predictive std'); plt.title('Predictive std distribution (NLL model)'); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b9472",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save kernels/results\n",
    "results = {\n",
    "    'baseline_kernel': str(gpr_base.kernel_),\n",
    "    'cv_kernel': str(getattr(gpr_cv, 'kernel_', 'n/a')),\n",
    "    'nll_kernel': str(getattr(gpr_nll, 'kernel_', 'n/a')),\n",
    "}\n",
    "import json\n",
    "with open('/mnt/data/gpr_tuning_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print('Saved /mnt/data/gpr_tuning_results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264e4aa8",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- If `skopt` is not installed, run `pip install scikit-optimize` then rerun the notebook.\n",
    "- `gp_minimize` runs multiple GPR fits and can be slow; reduce `n_calls` to speed up."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
