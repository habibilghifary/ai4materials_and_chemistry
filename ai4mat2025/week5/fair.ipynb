{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0523c17c",
   "metadata": {},
   "source": [
    "# Week5: Host a F.A.I.R. Dataset on Hugging Face\n",
    "\n",
    "### **TAs: Chiku Parida (chipa@dtu.dk)**\n",
    "\n",
    "**Outcome:** By the end, you'll publish a small *F.A.I.R.* dataset to the Hugging Face Hub and learn how others can discover, cite, and reuse it.\n",
    "\n",
    "We will:\n",
    "1. Structure a small materials dataset (CSV + JSONL + optional EXTXYZ)  \n",
    "2. Author a dataset card (README) with license, citation, and provenance  \n",
    "3. Validate data against a schema for *R*eusability  \n",
    "4. Upload to the Hub using `huggingface_hub`\n",
    "5. Load the dataset back for analysis\n",
    "\n",
    "> **Note:** You can adapt this template to large atomic datasets, polymer property tables, etc.\n",
    "\n",
    "\n",
    "| Optional Youtube Videos\n",
    "---\n",
    "| [Youtube Video - 1](https://youtu.be/VqqwTz1z1SE \"Youtube Video\")\n",
    "| [Youtube Video - 2](https://youtu.be/EVdYKvTdLqw \"Youtube Video\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38d6fc1",
   "metadata": {},
   "source": [
    "## 1) Install & Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfae5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, install\n",
    "# !pip install -q huggingface_hub datasets jsonschema ase pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064c6a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import login, HfApi, create_repo, upload_folder, hf_hub_download\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json, getpass, os\n",
    "from jsonschema import validate, Draft202012Validator\n",
    "from jsonschema.exceptions import ValidationError\n",
    "from ase import Atoms\n",
    "from ase.io import write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d020c",
   "metadata": {},
   "source": [
    "## 2) Log in to Hugging Face\n",
    "Create an account on https://huggingface.co if you don't have one.  \n",
    "Generate a User Access Token (Settings ‚Üí Access Tokens ‚Üí *New token*, scope: **Write**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4741d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'HF_TOKEN' not in os.environ:\n",
    "    token = getpass.getpass(\"Paste your Hugging Face token (will be hidden): \")\n",
    "    login(token=token)\n",
    "else:\n",
    "    login(token=os.environ['HF_TOKEN'])\n",
    "print(\"üîê Logged in to Hugging Face Hub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19d3cc5",
   "metadata": {},
   "source": [
    "## 3) Build a Tiny Example Materials Dataset (local)\n",
    "We'll prepare a small dataset about hypothetical oxide materials with (formula, system, bandgap) and optional atomic structures saved in EXTXYZ.\n",
    "\n",
    "Folder layout (**recommended**):\n",
    "```\n",
    "fair_dataset_demo/\n",
    "‚îú‚îÄ data/\n",
    "‚îÇ  ‚îú‚îÄ table.csv                 # tabular main data\n",
    "‚îÇ  ‚îú‚îÄ records.jsonl             # line-delimited JSON for provenance\n",
    "‚îÇ  ‚îî‚îÄ structures/\n",
    "‚îÇ     ‚îú‚îÄ ABO3_0001.xyz          # optional: atomic structures\n",
    "‚îÇ     ‚îî‚îÄ ...\n",
    "‚îú‚îÄ metadata/\n",
    "‚îÇ  ‚îú‚îÄ schema.json               # JSON Schema for validation\n",
    "‚îú‚îÄ LICENSE                      # compatible license\n",
    "‚îú‚îÄ CITATION.cff                 # citation info\n",
    "‚îî‚îÄ README.md                    # Dataset card with YAML frontmatter\n",
    "```\n",
    "üí° This structure is simple, discoverable, and compatible with the Hub's dataset hosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f1d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"fair_dataset_demo\")\n",
    "data_dir = root / \"data\"\n",
    "struct_dir = data_dir / \"structures\"\n",
    "meta_dir = root / \"metadata\"\n",
    "for p in [data_dir, struct_dir, meta_dir]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create a tiny CSV table\n",
    "rows = [\n",
    "    {\"id\": \"ABO3_0001\", \"formula\": \"SrTiO3\", \"system\": \"perovskite\", \"bandgap_eV\": 3.2, \"structure_path\": \"data/structures/ABO3_0001.xyz\"},\n",
    "    {\"id\": \"ABO3_0002\", \"formula\": \"BaZrO3\", \"system\": \"perovskite\", \"bandgap_eV\": 5.0, \"structure_path\": \"data/structures/ABO3_0002.xyz\"},\n",
    "    {\"id\": \"ABO3_0003\", \"formula\": \"LaAlO3\", \"system\": \"perovskite\", \"bandgap_eV\": 5.6, \"structure_path\": \"data/structures/ABO3_0003.xyz\"},\n",
    "]\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(data_dir/\"table.csv\", index=False)\n",
    "\n",
    "# Mirror as JSONL\n",
    "with open(data_dir/\"records.jsonl\", \"w\") as f:\n",
    "    for r in rows:\n",
    "        f.write(json.dumps(r) + \"\\n\")\n",
    "\n",
    "# Create tiny example structures (cubic) and save as EXTXYZ \n",
    "a = 3.905  # ~CaTiO3\n",
    "for i, r in enumerate(rows, start=1):\n",
    "    # Simple cubic ABO3 placeholder atoms\n",
    "    atoms = Atoms(symbols=\"CaTiO3\", positions=[(0,0,0), (a/2,a/2,a/2), (a/2,0,0), (0,a/2,0), (0,0,a/2)], cell=[a,a,a], pbc=True)\n",
    "    atoms.info.update({\n",
    "        \"id\": r[\"id\"],\n",
    "        \"formula\": r[\"formula\"],\n",
    "        \"system\": r[\"system\"],\n",
    "        \"bandgap_eV\": r[\"bandgap_eV\"],\n",
    "    })\n",
    "    write(struct_dir/f\"{r['id']}.xyz\", atoms)\n",
    "\n",
    "print(\"üì¶ Local example dataset created at:\", root.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb91c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "  \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
    "  \"title\": \"SimpleMaterialsRow\",\n",
    "  \"type\": \"object\",\n",
    "  \"required\": [\"id\", \"formula\", \"system\", \"bandgap_eV\", \"structure_path\"],\n",
    "  \"properties\": {\n",
    "    \"id\": {\"type\": \"string\"},\n",
    "    \"formula\": {\"type\": \"string\"},\n",
    "    \"system\": {\"type\": \"string\", \"enum\": [\"perovskite\", \"spinel\", \"rocksalt\", \"other\"]},\n",
    "    \"bandgap_eV\": {\"type\": \"number\", \"minimum\": 0},\n",
    "    \"structure_path\": {\"type\": \"string\"}\n",
    "  },\n",
    "  \"additionalProperties\": False\n",
    "}\n",
    "(meta_dir/\"schema.jsonschema\").write_text(json.dumps(schema, indent=2))\n",
    "\n",
    "# Validate the CSV rows\n",
    "validator = Draft202012Validator(schema)\n",
    "errors = []\n",
    "for i, r in df.iterrows():\n",
    "    try:\n",
    "        validator.validate(r.to_dict())\n",
    "    except ValidationError as e:\n",
    "        errors.append((i, str(e)))\n",
    "if errors:\n",
    "    print(\"‚ùå Validation errors:\")\n",
    "    for ix, msg in errors:\n",
    "        print(ix, msg)\n",
    "else:\n",
    "    print(\"All rows pass schema validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddc3ec7",
   "metadata": {},
   "source": [
    "## 4) Create a Dataset Repo on the Hub & Upload\n",
    "\n",
    "- **Name:** `fair_dataset_demo`  \n",
    "- Set `private=False` for teaching visibility (or `True` until review)  \n",
    "- `upload_folder` handles large files without requiring Git LFS locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f75ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "username = api.whoami()['name']\n",
    "repo_name = \"fair_dataset_demo\"\n",
    "repo_id = f\"{username}/{repo_name}\"\n",
    "\n",
    "# Create (idempotent): if exists, do nothing\n",
    "create_repo(repo_id=repo_id, repo_type=\"dataset\", private=False, exist_ok=True)\n",
    "\n",
    "# Upload entire folder\n",
    "upload_folder(\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\",\n",
    "    folder_path=str(root),\n",
    "    path_in_repo=\".\",\n",
    ")\n",
    "print(f\"üöÄ Uploaded to https://huggingface.co/datasets/{repo_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0014ac",
   "metadata": {},
   "source": [
    "## 5) Verify by Loading from the Hub\n",
    "Demonstrate both `datasets` loading and direct file download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d90bc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Attempting to load the CSV via datasets...\")\n",
    "ds = load_dataset(\"cparidaAI/fair_dataset_demo\", data_files={\"train\": \"data/table.csv\"})\n",
    "display(ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: use subfolder + bare filename\n",
    "xyz_path = hf_hub_download(\n",
    "    repo_id=r\"cparidaAI/fair_dataset_demo\",\n",
    "    repo_type=\"dataset\",\n",
    "    subfolder=\"data/structures\",\n",
    "    filename=\"ABO3_0001.xyz\",\n",
    ")\n",
    "print(\"Downloaded to:\", xyz_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173572f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "repo_id   = \"cparidaAI/fair_dataset_demo\"\n",
    "targetdir = \"/local/path/downloaded_data\"\n",
    "\n",
    "local_path = snapshot_download(\n",
    "    repo_id=\"cparidaAI/fair_dataset_demo\",\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=targetdir,\n",
    "    local_dir_use_symlinks=False,\n",
    "    allow_patterns=[\"data/**\"],     # only grab files under data/\n",
    ")\n",
    "print(\"Downloaded data/* to:\", local_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ce576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc49b523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ino_bg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
